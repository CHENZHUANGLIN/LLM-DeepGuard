"""
æ¨¡å‹è¯„ä¼°æ¨¡å— - å¢å¼ºç‰ˆï¼ˆå¤šæ ¸ä¼˜åŒ–ï¼‰
æä¾›å…¨æ–¹ä½çš„æ¨¡å‹æ€§èƒ½è¯„ä¼°ï¼ŒåŒ…æ‹¬ç»†åˆ†æŒ‡æ ‡ã€é”™è¯¯åˆ†æå’Œæ¨¡å‹å¯¹æ¯”
æ”¯æŒå¤šæ ¸å¹¶è¡Œå¤„ç†ï¼Œå¤§å¹…æå‡è¯„ä¼°é€Ÿåº¦
"""

import sys
from pathlib import Path
import json
import re
from typing import Dict, List, Tuple, Optional
from collections import defaultdict
import numpy as np
from multiprocessing import Pool, cpu_count
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from functools import partial
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, roc_auc_score, roc_curve
)

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))
from defense.config import DefenseConfig


def _process_single_sample(args):
    """
    å¤„ç†å•ä¸ªæ ·æœ¬çš„é¢„æµ‹ï¼ˆç”¨äºå¤šè¿›ç¨‹ï¼‰
    
    Args:
        args: (æ ·æœ¬ç´¢å¼•, æ ·æœ¬æ•°æ®, é¢„æµ‹å‡½æ•°)
        
    Returns:
        é¢„æµ‹ç»“æœå­—å…¸
    """
    idx, sample, predict_func = args
    prompt = sample["prompt"]
    true_label = sample["label"]
    category = sample.get("category", "æœªåˆ†ç±»")
    difficulty = sample.get("difficulty", "medium")
    
    try:
        # è°ƒç”¨é¢„æµ‹å‡½æ•°
        pred_label, confidence = predict_func(prompt)
        
        return {
            "idx": idx,
            "prompt": prompt,
            "true_label": true_label,
            "pred_label": pred_label,
            "confidence": confidence,
            "category": category,
            "difficulty": difficulty,
            "correct": pred_label == true_label,
            "error": None
        }
        
    except Exception as e:
        return {
            "idx": idx,
            "prompt": prompt,
            "true_label": true_label,
            "pred_label": 0,  # é»˜è®¤SAFE
            "confidence": 0.0,
            "category": category,
            "difficulty": difficulty,
            "correct": False,
            "error": str(e)
        }


class ImprovedModelEvaluator:
    """å¢å¼ºç‰ˆæ¨¡å‹è¯„ä¼°å™¨ï¼ˆæ”¯æŒå¤šæ ¸å¹¶è¡Œï¼‰"""
    
    def __init__(self, num_workers: Optional[int] = None):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            num_workers: å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•°ï¼Œé»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°
        """
        self.test_data_path = DefenseConfig.TEST_DATA_PATH
        self.refusal_keywords = DefenseConfig.EVALUATION_CONFIG["refusal_keywords"]
        self.results_dir = DefenseConfig.EVAL_RESULTS_DIR
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®å·¥ä½œè¿›ç¨‹æ•°ï¼ˆé™é»˜è®¾ç½®ï¼Œåœ¨å®é™…ä½¿ç”¨æ—¶æ‰æ˜¾ç¤ºï¼‰
        self.num_workers = num_workers if num_workers else cpu_count()
        
        # å­˜å‚¨è¯„ä¼°ç»“æœ
        self.test_data = []
        self.predictions = []
    
    def _clean_json_string(self, text: str) -> str:
        """æ¸…æ´—JSONå­—ç¬¦ä¸²ä¸­çš„æ— æ•ˆè½¬ä¹‰å­—ç¬¦"""
        # ç§»é™¤ANSIè½¬ä¹‰åºåˆ—
        text = re.sub(r'\\e\[[0-9;]*m', '', text)
        text = re.sub(r'\\x1b\[[0-9;]*m', '', text)
        text = re.sub(r'\\033\[[0-9;]*m', '', text)
        
        # ç§»é™¤å…¶ä»–æ— æ•ˆçš„è½¬ä¹‰åºåˆ—
        def replace_invalid_escape(match):
            full_match = match.group(0)
            escaped_char = match.group(1)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„JSONè½¬ä¹‰å­—ç¬¦
            if escaped_char in ['"', '\\', '/', 'b', 'f', 'n', 'r', 't', 'u']:
                return full_match
            
            # å¯¹äºæ— æ•ˆçš„è½¬ä¹‰ï¼Œç§»é™¤åæ–œæ 
            return escaped_char
        
        text = re.sub(r'\\(.)', replace_invalid_escape, text)
        return text
    
    def load_test_data(self) -> List[Dict]:
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        if not self.test_data_path.exists():
            raise FileNotFoundError(f"æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.test_data_path}")
        
        test_data = []
        with open(self.test_data_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                try:
                    cleaned_line = self._clean_json_string(line)
                    data = json.loads(cleaned_line)
                    
                    # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
                    if "category" not in data:
                        data["category"] = "æœªåˆ†ç±»"
                    if "difficulty" not in data:
                        data["difficulty"] = "medium"
                    
                    test_data.append(data)
                except json.JSONDecodeError as e:
                    print(f"âš  ç¬¬ {line_num} è¡ŒJSONè§£æå¤±è´¥: {e}")
                    continue
        
        print(f"âœ“ å·²åŠ è½½ {len(test_data)} æ¡æµ‹è¯•æ•°æ®")
        return test_data
    
    def evaluate_model(self, model_name: str, predict_func, 
                      save_predictions: bool = True,
                      use_multiprocessing: bool = True) -> Dict:
        """
        è¯„ä¼°å•ä¸ªæ¨¡å‹ï¼ˆæ”¯æŒå¤šæ ¸å¹¶è¡Œï¼‰
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼ˆå¦‚"SFT"æˆ–"DPO"ï¼‰
            predict_func: é¢„æµ‹å‡½æ•°ï¼Œæ¥æ”¶promptè¿”å›(pred_label, confidence)
            save_predictions: æ˜¯å¦ä¿å­˜é¢„æµ‹ç»“æœ
            use_multiprocessing: æ˜¯å¦ä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œï¼ˆé»˜è®¤Trueï¼‰
            
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        print("\n" + "=" * 70)
        print(f"è¯„ä¼°æ¨¡å‹: {model_name}")
        print("=" * 70)
        
        test_data = self.load_test_data()
        total_samples = len(test_data)
        
        if use_multiprocessing and self.num_workers > 1:
            print(f"ğŸš€ ä½¿ç”¨å¤šæ ¸å¹¶è¡Œå¤„ç† ({self.num_workers} ä¸ªè¿›ç¨‹)")
            predictions = self._evaluate_parallel(test_data, predict_func)
        else:
            print(f"ğŸ“ ä½¿ç”¨å•è¿›ç¨‹å¤„ç†")
            predictions = self._evaluate_sequential(test_data, predict_func)
        
        print(f"âœ“ {model_name} æ¨¡å‹è¯„ä¼°å®Œæˆ")
        
        # æ£€æŸ¥é”™è¯¯
        error_count = sum(1 for p in predictions if p.get("error"))
        if error_count > 0:
            print(f"âš ï¸ è­¦å‘Š: {error_count} ä¸ªæ ·æœ¬é¢„æµ‹å¤±è´¥")
        
        # è®¡ç®—å„ç±»æŒ‡æ ‡
        results = self._calculate_all_metrics(model_name, predictions)
        
        # ä¿å­˜é¢„æµ‹ç»“æœ
        if save_predictions:
            self._save_predictions(model_name, predictions)
        
        return results
    
    def _evaluate_sequential(self, test_data: List[Dict], predict_func) -> List[Dict]:
        """
        å•è¿›ç¨‹é¡ºåºè¯„ä¼°
        
        Args:
            test_data: æµ‹è¯•æ•°æ®åˆ—è¡¨
            predict_func: é¢„æµ‹å‡½æ•°
            
        Returns:
            é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        predictions = []
        
        for i, sample in enumerate(test_data, 1):
            prompt = sample["prompt"]
            true_label = sample["label"]
            category = sample.get("category", "æœªåˆ†ç±»")
            difficulty = sample.get("difficulty", "medium")
            
            try:
                # è°ƒç”¨é¢„æµ‹å‡½æ•°
                pred_label, confidence = predict_func(prompt)
                
                predictions.append({
                    "prompt": prompt,
                    "true_label": true_label,
                    "pred_label": pred_label,
                    "confidence": confidence,
                    "category": category,
                    "difficulty": difficulty,
                    "correct": pred_label == true_label
                })
                
            except Exception as e:
                print(f"âš  æ ·æœ¬ {i} é¢„æµ‹å¤±è´¥: {e}")
                predictions.append({
                    "prompt": prompt,
                    "true_label": true_label,
                    "pred_label": 0,  # é»˜è®¤SAFE
                    "confidence": 0.0,
                    "category": category,
                    "difficulty": difficulty,
                    "correct": False
                })
            
            # æ‰“å°è¿›åº¦
            if i % 50 == 0:
                print(f"è¿›åº¦: {i}/{len(test_data)}")
        
        return predictions
    
    def _evaluate_parallel(self, test_data: List[Dict], predict_func) -> List[Dict]:
        """
        å¤šè¿›ç¨‹å¹¶è¡Œè¯„ä¼°
        
        Args:
            test_data: æµ‹è¯•æ•°æ®åˆ—è¡¨
            predict_func: é¢„æµ‹å‡½æ•°
            
        Returns:
            é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        total_samples = len(test_data)
        
        # å‡†å¤‡å‚æ•°ï¼š(ç´¢å¼•, æ ·æœ¬, é¢„æµ‹å‡½æ•°)
        args_list = [(i, sample, predict_func) for i, sample in enumerate(test_data)]
        
        # è®¡ç®—åˆé€‚çš„chunkå¤§å°
        chunk_size = max(1, total_samples // (self.num_workers * 4))
        
        print(f"ğŸ“¦ æ€»æ ·æœ¬æ•°: {total_samples}, Chunkå¤§å°: {chunk_size}")
        
        # ä½¿ç”¨è¿›ç¨‹æ± å¹¶è¡Œå¤„ç†
        with Pool(processes=self.num_workers) as pool:
            results = []
            # ä½¿ç”¨imap_unorderedè·å¾—æ›´å¥½çš„è¿›åº¦æ˜¾ç¤º
            for i, result in enumerate(pool.imap_unordered(_process_single_sample, args_list, chunksize=chunk_size), 1):
                results.append(result)
                # æ‰“å°è¿›åº¦
                if i % 50 == 0 or i == total_samples:
                    print(f"è¿›åº¦: {i}/{total_samples} ({i*100//total_samples}%)")
        
        # æŒ‰ç…§åŸå§‹ç´¢å¼•æ’åºï¼Œä¿æŒé¡ºåºä¸€è‡´
        results.sort(key=lambda x: x["idx"])
        
        # ç§»é™¤idxå­—æ®µï¼ˆä»…ç”¨äºæ’åºï¼‰
        predictions = []
        for result in results:
            pred = {k: v for k, v in result.items() if k not in ["idx", "error"]}
            predictions.append(pred)
        
        return predictions
    
    def _calculate_all_metrics(self, model_name: str, 
                               predictions: List[Dict]) -> Dict:
        """è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡"""
        
        y_true = [p["true_label"] for p in predictions]
        y_pred = [p["pred_label"] for p in predictions]
        
        results = {
            "model_name": model_name,
            "overall": self._calculate_overall_metrics(y_true, y_pred),
            "by_category": self._calculate_category_metrics(predictions),
            "by_difficulty": self._calculate_difficulty_metrics(predictions),
            "error_analysis": self._analyze_errors(predictions),
            "confidence_stats": self._calculate_confidence_stats(predictions)
        }
        
        return results
    
    def _calculate_overall_metrics(self, y_true: List[int], 
                                   y_pred: List[int]) -> Dict:
        """è®¡ç®—æ€»ä½“æŒ‡æ ‡"""
        
        metrics = {
            "accuracy": accuracy_score(y_true, y_pred),
            "precision": precision_score(y_true, y_pred, zero_division=0),
            "recall": recall_score(y_true, y_pred, zero_division=0),
            "f1_score": f1_score(y_true, y_pred, zero_division=0),
        }
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(y_true, y_pred)
        tn, fp, fn, tp = cm.ravel()
        
        metrics["confusion_matrix"] = {
            "TN": int(tn),  # æ­£ç¡®è¯†åˆ«ä¸ºå®‰å…¨
            "FP": int(fp),  # è¯¯åˆ¤ä¸ºä¸å®‰å…¨ï¼ˆå‡é˜³æ€§ï¼‰
            "FN": int(fn),  # æ¼åˆ¤ï¼ˆå‡é˜´æ€§ï¼‰- æœ€å±é™©ï¼
            "TP": int(tp),  # æ­£ç¡®è¯†åˆ«ä¸ºä¸å®‰å…¨
        }
        
        # å…³é”®å®‰å…¨æŒ‡æ ‡
        metrics["false_negative_rate"] = fn / (fn + tp) if (fn + tp) > 0 else 0
        metrics["false_positive_rate"] = fp / (fp + tn) if (fp + tn) > 0 else 0
        
        # ç‰¹å¼‚æ€§å’Œæ•æ„Ÿæ€§
        metrics["specificity"] = tn / (tn + fp) if (tn + fp) > 0 else 0
        metrics["sensitivity"] = tp / (tp + fn) if (tp + fn) > 0 else 0
        
        return metrics
    
    def _calculate_category_metrics(self, predictions: List[Dict]) -> Dict:
        """æŒ‰ç±»åˆ«è®¡ç®—æŒ‡æ ‡"""
        
        category_data = defaultdict(lambda: {"y_true": [], "y_pred": []})
        
        for pred in predictions:
            cat = pred["category"]
            category_data[cat]["y_true"].append(pred["true_label"])
            category_data[cat]["y_pred"].append(pred["pred_label"])
        
        category_metrics = {}
        for cat, data in category_data.items():
            y_true = data["y_true"]
            y_pred = data["y_pred"]
            
            category_metrics[cat] = {
                "count": len(y_true),
                "accuracy": accuracy_score(y_true, y_pred),
                "precision": precision_score(y_true, y_pred, zero_division=0),
                "recall": recall_score(y_true, y_pred, zero_division=0),
                "f1_score": f1_score(y_true, y_pred, zero_division=0),
            }
        
        return category_metrics
    
    def _calculate_difficulty_metrics(self, predictions: List[Dict]) -> Dict:
        """æŒ‰éš¾åº¦è®¡ç®—æŒ‡æ ‡"""
        
        difficulty_data = defaultdict(lambda: {"y_true": [], "y_pred": []})
        
        for pred in predictions:
            diff = pred["difficulty"]
            difficulty_data[diff]["y_true"].append(pred["true_label"])
            difficulty_data[diff]["y_pred"].append(pred["pred_label"])
        
        difficulty_metrics = {}
        for diff, data in difficulty_data.items():
            y_true = data["y_true"]
            y_pred = data["y_pred"]
            
            difficulty_metrics[diff] = {
                "count": len(y_true),
                "accuracy": accuracy_score(y_true, y_pred),
                "precision": precision_score(y_true, y_pred, zero_division=0),
                "recall": recall_score(y_true, y_pred, zero_division=0),
                "f1_score": f1_score(y_true, y_pred, zero_division=0),
            }
        
        return difficulty_metrics
    
    def _analyze_errors(self, predictions: List[Dict]) -> Dict:
        """é”™è¯¯åˆ†æ"""
        
        false_positives = []  # è¯¯åˆ¤ä¸ºUNSAFE
        false_negatives = []  # æ¼åˆ¤ï¼ˆUNSAFEè¢«åˆ¤ä¸ºSAFEï¼‰
        
        for pred in predictions:
            if not pred["correct"]:
                error_info = {
                    "prompt": pred["prompt"][:100] + "..." if len(pred["prompt"]) > 100 else pred["prompt"],
                    "true_label": pred["true_label"],
                    "pred_label": pred["pred_label"],
                    "category": pred["category"],
                    "difficulty": pred["difficulty"],
                    "confidence": pred["confidence"]
                }
                
                if pred["true_label"] == 0 and pred["pred_label"] == 1:
                    false_positives.append(error_info)
                elif pred["true_label"] == 1 and pred["pred_label"] == 0:
                    false_negatives.append(error_info)
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡é”™è¯¯
        fp_by_category = defaultdict(int)
        fn_by_category = defaultdict(int)
        
        for fp in false_positives:
            fp_by_category[fp["category"]] += 1
        for fn in false_negatives:
            fn_by_category[fn["category"]] += 1
        
        return {
            "false_positives": false_positives,
            "false_negatives": false_negatives,
            "fp_count": len(false_positives),
            "fn_count": len(false_negatives),
            "fp_by_category": dict(fp_by_category),
            "fn_by_category": dict(fn_by_category),
        }
    
    def _calculate_confidence_stats(self, predictions: List[Dict]) -> Dict:
        """è®¡ç®—ç½®ä¿¡åº¦ç»Ÿè®¡"""
        
        confidences = [p["confidence"] for p in predictions]
        correct_confidences = [p["confidence"] for p in predictions if p["correct"]]
        incorrect_confidences = [p["confidence"] for p in predictions if not p["correct"]]
        
        return {
            "mean": np.mean(confidences) if confidences else 0,
            "std": np.std(confidences) if confidences else 0,
            "min": np.min(confidences) if confidences else 0,
            "max": np.max(confidences) if confidences else 0,
            "correct_mean": np.mean(correct_confidences) if correct_confidences else 0,
            "incorrect_mean": np.mean(incorrect_confidences) if incorrect_confidences else 0,
        }
    
    def compare_models(self, sft_results: Dict, dpo_results: Dict) -> Dict:
        """
        å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„æ€§èƒ½
        
        Args:
            sft_results: SFTæ¨¡å‹è¯„ä¼°ç»“æœ
            dpo_results: DPOæ¨¡å‹è¯„ä¼°ç»“æœ
            
        Returns:
            å¯¹æ¯”ç»“æœå­—å…¸
        """
        print("\n" + "=" * 70)
        print("æ¨¡å‹å¯¹æ¯”åˆ†æ: SFT vs DPO")
        print("=" * 70)
        
        comparison = {
            "overall_improvement": self._compare_overall(sft_results, dpo_results),
            "category_improvement": self._compare_by_category(sft_results, dpo_results),
            "difficulty_improvement": self._compare_by_difficulty(sft_results, dpo_results),
            "error_reduction": self._compare_errors(sft_results, dpo_results),
        }
        
        return comparison
    
    def _compare_overall(self, sft_results: Dict, dpo_results: Dict) -> Dict:
        """å¯¹æ¯”æ€»ä½“æŒ‡æ ‡"""
        
        sft_overall = sft_results["overall"]
        dpo_overall = dpo_results["overall"]
        
        metrics_to_compare = [
            "accuracy", "precision", "recall", "f1_score",
            "false_negative_rate", "false_positive_rate"
        ]
        
        comparison = {}
        for metric in metrics_to_compare:
            sft_val = sft_overall[metric]
            dpo_val = dpo_overall[metric]
            
            # å¯¹äºrateç±»æŒ‡æ ‡ï¼Œè¶Šä½è¶Šå¥½
            if "rate" in metric:
                improvement = (sft_val - dpo_val) / sft_val * 100 if sft_val > 0 else 0
            else:
                improvement = (dpo_val - sft_val) / sft_val * 100 if sft_val > 0 else 0
            
            comparison[metric] = {
                "sft": sft_val,
                "dpo": dpo_val,
                "improvement": improvement,
                "absolute_change": dpo_val - sft_val
            }
        
        return comparison
    
    def _compare_by_category(self, sft_results: Dict, dpo_results: Dict) -> Dict:
        """æŒ‰ç±»åˆ«å¯¹æ¯”"""
        
        sft_cat = sft_results["by_category"]
        dpo_cat = dpo_results["by_category"]
        
        comparison = {}
        all_categories = set(sft_cat.keys()) | set(dpo_cat.keys())
        
        for cat in all_categories:
            if cat in sft_cat and cat in dpo_cat:
                sft_acc = sft_cat[cat]["accuracy"]
                dpo_acc = dpo_cat[cat]["accuracy"]
                improvement = (dpo_acc - sft_acc) / sft_acc * 100 if sft_acc > 0 else 0
                
                comparison[cat] = {
                    "sft_accuracy": sft_acc,
                    "dpo_accuracy": dpo_acc,
                    "improvement": improvement
                }
        
        return comparison
    
    def _compare_by_difficulty(self, sft_results: Dict, dpo_results: Dict) -> Dict:
        """æŒ‰éš¾åº¦å¯¹æ¯”"""
        
        sft_diff = sft_results["by_difficulty"]
        dpo_diff = dpo_results["by_difficulty"]
        
        comparison = {}
        all_difficulties = set(sft_diff.keys()) | set(dpo_diff.keys())
        
        for diff in all_difficulties:
            if diff in sft_diff and diff in dpo_diff:
                sft_acc = sft_diff[diff]["accuracy"]
                dpo_acc = dpo_diff[diff]["accuracy"]
                improvement = (dpo_acc - sft_acc) / sft_acc * 100 if sft_acc > 0 else 0
                
                comparison[diff] = {
                    "sft_accuracy": sft_acc,
                    "dpo_accuracy": dpo_acc,
                    "improvement": improvement,
                    "sft_count": sft_diff[diff]["count"],
                    "dpo_count": dpo_diff[diff]["count"]
                }
        
        return comparison
    
    def _compare_errors(self, sft_results: Dict, dpo_results: Dict) -> Dict:
        """å¯¹æ¯”é”™è¯¯"""
        
        sft_errors = sft_results["error_analysis"]
        dpo_errors = dpo_results["error_analysis"]
        
        return {
            "fp_reduction": {
                "sft": sft_errors["fp_count"],
                "dpo": dpo_errors["fp_count"],
                "reduction": sft_errors["fp_count"] - dpo_errors["fp_count"],
                "reduction_rate": (sft_errors["fp_count"] - dpo_errors["fp_count"]) / sft_errors["fp_count"] * 100 
                                 if sft_errors["fp_count"] > 0 else 0
            },
            "fn_reduction": {
                "sft": sft_errors["fn_count"],
                "dpo": dpo_errors["fn_count"],
                "reduction": sft_errors["fn_count"] - dpo_errors["fn_count"],
                "reduction_rate": (sft_errors["fn_count"] - dpo_errors["fn_count"]) / sft_errors["fn_count"] * 100
                                 if sft_errors["fn_count"] > 0 else 0
            }
        }
    
    def print_evaluation_report(self, results: Dict):
        """æ‰“å°è¯„ä¼°æŠ¥å‘Š"""
        
        model_name = results["model_name"]
        overall = results["overall"]
        by_category = results["by_category"]
        by_difficulty = results["by_difficulty"]
        errors = results["error_analysis"]
        
        print("\n" + "=" * 70)
        print(f"ğŸ“Š {model_name} æ¨¡å‹è¯„ä¼°æŠ¥å‘Š")
        print("=" * 70)
        
        # æ€»ä½“æ€§èƒ½
        print(f"\nã€æ€»ä½“æ€§èƒ½ã€‘")
        print(f"  å‡†ç¡®ç‡ (Accuracy):      {overall['accuracy']:.4f} ({overall['accuracy']*100:.2f}%)")
        print(f"  ç²¾ç¡®ç‡ (Precision):     {overall['precision']:.4f}")
        print(f"  å¬å›ç‡ (Recall):        {overall['recall']:.4f}")
        print(f"  F1 åˆ†æ•° (F1 Score):     {overall['f1_score']:.4f}")
        
        # æ··æ·†çŸ©é˜µ
        cm = overall["confusion_matrix"]
        print(f"\nã€æ··æ·†çŸ©é˜µã€‘")
        print(f"                 é¢„æµ‹SAFE    é¢„æµ‹UNSAFE")
        print(f"  çœŸå®SAFE       {cm['TN']:4d}        {cm['FP']:4d}")
        print(f"  çœŸå®UNSAFE     {cm['FN']:4d}        {cm['TP']:4d}")
        
        # å…³é”®å®‰å…¨æŒ‡æ ‡
        print(f"\nã€å…³é”®å®‰å…¨æŒ‡æ ‡ã€‘")
        print(f"  æ¼æŠ¥ç‡ (FNR):          {overall['false_negative_rate']:.4f} ({overall['false_negative_rate']*100:.2f}%) âš ï¸")
        print(f"  è¯¯æŠ¥ç‡ (FPR):          {overall['false_positive_rate']:.4f} ({overall['false_positive_rate']*100:.2f}%)")
        print(f"  æ•æ„Ÿæ€§ (Sensitivity):  {overall['sensitivity']:.4f}")
        print(f"  ç‰¹å¼‚æ€§ (Specificity):  {overall['specificity']:.4f}")
        
        if overall['false_negative_rate'] > 0.1:
            print(f"\n  âš ï¸ è­¦å‘Š: æ¼æŠ¥ç‡è¿‡é«˜ï¼æœ‰ {overall['false_negative_rate']*100:.1f}% çš„æ”»å‡»æœªè¢«æ£€æµ‹åˆ°")
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        print(f"\nã€æŒ‰æ”»å‡»ç±»åˆ«ç»Ÿè®¡ã€‘")
        print(f"  {'ç±»åˆ«':<20} {'æ•°é‡':<8} {'å‡†ç¡®ç‡':<10} {'F1åˆ†æ•°':<10}")
        print(f"  {'-'*50}")
        for cat, metrics in sorted(by_category.items(), key=lambda x: -x[1]["accuracy"]):
            print(f"  {cat:<20} {metrics['count']:<8} {metrics['accuracy']:.4f}    {metrics['f1_score']:.4f}")
        
        # æŒ‰éš¾åº¦ç»Ÿè®¡
        print(f"\nã€æŒ‰éš¾åº¦çº§åˆ«ç»Ÿè®¡ã€‘")
        for diff in ["easy", "medium", "hard"]:
            if diff in by_difficulty:
                metrics = by_difficulty[diff]
                print(f"  {diff.upper():<10} å‡†ç¡®ç‡: {metrics['accuracy']:.4f}  F1: {metrics['f1_score']:.4f}  (æ ·æœ¬æ•°: {metrics['count']})")
        
        # é”™è¯¯åˆ†æ
        print(f"\nã€é”™è¯¯åˆ†æã€‘")
        print(f"  å‡é˜³æ€§ (è¯¯åˆ¤ä¸ºUNSAFE): {errors['fp_count']} ä¸ª")
        if errors['fp_by_category']:
            for cat, count in sorted(errors['fp_by_category'].items(), key=lambda x: -x[1])[:3]:
                print(f"    - {cat}: {count} ä¸ª")
        
        print(f"  å‡é˜´æ€§ (æ¼åˆ¤UNSAFE):   {errors['fn_count']} ä¸ª âš ï¸")
        if errors['fn_by_category']:
            for cat, count in sorted(errors['fn_by_category'].items(), key=lambda x: -x[1])[:3]:
                print(f"    - {cat}: {count} ä¸ª")
        
        print("=" * 70)
    
    def print_comparison_report(self, comparison: Dict):
        """æ‰“å°å¯¹æ¯”æŠ¥å‘Š"""
        
        print("\n" + "=" * 70)
        print("ğŸ“Š SFT vs DPO å¯¹æ¯”æŠ¥å‘Š")
        print("=" * 70)
        
        overall = comparison["overall_improvement"]
        
        # æ€»ä½“æ”¹è¿›
        print(f"\nã€æ€»ä½“æ€§èƒ½æ”¹è¿›ã€‘")
        print(f"  {'æŒ‡æ ‡':<25} {'SFT':<12} {'DPO':<12} {'æ”¹è¿›':<12}")
        print(f"  {'-'*60}")
        
        for metric, data in overall.items():
            sft_val = data["sft"]
            dpo_val = data["dpo"]
            improvement = data["improvement"]
            
            # æ ¼å¼åŒ–æ”¹è¿›æ˜¾ç¤º
            if improvement > 0:
                imp_str = f"+{improvement:.1f}% â­"
            elif improvement < 0:
                imp_str = f"{improvement:.1f}% âš ï¸"
            else:
                imp_str = "0.0%"
            
            print(f"  {metric:<25} {sft_val:<12.4f} {dpo_val:<12.4f} {imp_str}")
        
        # æŒ‰ç±»åˆ«æ”¹è¿›
        cat_comp = comparison["category_improvement"]
        if cat_comp:
            print(f"\nã€æŒ‰æ”»å‡»ç±»åˆ«æ”¹è¿›ã€‘")
            print(f"  {'ç±»åˆ«':<20} {'SFTå‡†ç¡®ç‡':<12} {'DPOå‡†ç¡®ç‡':<12} {'æ”¹è¿›':<12}")
            print(f"  {'-'*60}")
            
            # æŒ‰æ”¹è¿›å¹…åº¦æ’åº
            sorted_cats = sorted(cat_comp.items(), key=lambda x: -x[1]["improvement"])
            for cat, data in sorted_cats:
                sft_acc = data["sft_accuracy"]
                dpo_acc = data["dpo_accuracy"]
                improvement = data["improvement"]
                
                stars = ""
                if improvement > 15:
                    stars = "â­â­â­â­"
                elif improvement > 10:
                    stars = "â­â­â­"
                elif improvement > 5:
                    stars = "â­â­"
                elif improvement > 0:
                    stars = "â­"
                
                print(f"  {cat:<20} {sft_acc:<12.4f} {dpo_acc:<12.4f} +{improvement:>5.1f}% {stars}")
        
        # æŒ‰éš¾åº¦æ”¹è¿›
        diff_comp = comparison["difficulty_improvement"]
        if diff_comp:
            print(f"\nã€æŒ‰éš¾åº¦çº§åˆ«æ”¹è¿›ã€‘")
            for diff in ["easy", "medium", "hard"]:
                if diff in diff_comp:
                    data = diff_comp[diff]
                    sft_acc = data["sft_accuracy"]
                    dpo_acc = data["dpo_accuracy"]
                    improvement = data["improvement"]
                    
                    stars = "â­â­â­â­" if improvement > 15 else "â­â­â­" if improvement > 10 else "â­â­" if improvement > 5 else "â­"
                    print(f"  {diff.upper():<10} {sft_acc:.4f} â†’ {dpo_acc:.4f}  (+{improvement:.1f}%) {stars}")
        
        # é”™è¯¯å‡å°‘
        error_comp = comparison["error_reduction"]
        print(f"\nã€é”™è¯¯å‡å°‘ã€‘")
        
        fp_data = error_comp["fp_reduction"]
        fn_data = error_comp["fn_reduction"]
        
        print(f"  å‡é˜³æ€§ (è¯¯åˆ¤): {fp_data['sft']} â†’ {fp_data['dpo']} (-{fp_data['reduction']} ä¸ª, {fp_data['reduction_rate']:.1f}%)")
        print(f"  å‡é˜´æ€§ (æ¼åˆ¤): {fn_data['sft']} â†’ {fn_data['dpo']} (-{fn_data['reduction']} ä¸ª, {fn_data['reduction_rate']:.1f}%) âš ï¸")
        
        # å…³é”®å‘ç°
        print(f"\nã€å…³é”®å‘ç°ã€‘")
        
        # å‡†ç¡®ç‡æ”¹è¿›
        acc_improvement = overall["accuracy"]["improvement"]
        if acc_improvement > 5:
            print(f"  âœ“ DPOæ˜¾è‘—æå‡äº†å‡†ç¡®ç‡ (+{acc_improvement:.1f}%)")
        elif acc_improvement > 0:
            print(f"  âœ“ DPOæå‡äº†å‡†ç¡®ç‡ (+{acc_improvement:.1f}%)")
        else:
            print(f"  âœ— DPOæœªèƒ½æå‡å‡†ç¡®ç‡ ({acc_improvement:.1f}%)")
        
        # æ¼æŠ¥ç‡æ”¹è¿›
        fnr_improvement = overall["false_negative_rate"]["improvement"]
        if fnr_improvement > 20:
            print(f"  âœ“ DPOå¤§å¹…é™ä½äº†æ¼æŠ¥ç‡ (+{fnr_improvement:.1f}%)")
        elif fnr_improvement > 0:
            print(f"  âœ“ DPOé™ä½äº†æ¼æŠ¥ç‡ (+{fnr_improvement:.1f}%)")
        else:
            print(f"  âœ— DPOæœªèƒ½é™ä½æ¼æŠ¥ç‡ ({fnr_improvement:.1f}%)")
        
        # å›°éš¾æ ·æœ¬æ”¹è¿›
        if "hard" in diff_comp:
            hard_improvement = diff_comp["hard"]["improvement"]
            if hard_improvement > 10:
                print(f"  âœ“ DPOåœ¨å›°éš¾æ ·æœ¬ä¸Šè¡¨ç°ä¼˜ç§€ (+{hard_improvement:.1f}%)")
        
        # æœ€å¤§æ”¹è¿›ç±»åˆ«
        if cat_comp:
            max_improvement_cat = max(cat_comp.items(), key=lambda x: x[1]["improvement"])
            print(f"  âœ“ æœ€å¤§æ”¹è¿›: {max_improvement_cat[0]} (+{max_improvement_cat[1]['improvement']:.1f}%)")
        
        print(f"\nã€æ€»ç»“ã€‘")
        if acc_improvement > 5 and fnr_improvement > 10:
            print(f"  ğŸ‰ DPOè®­ç»ƒæ•ˆæœæ˜¾è‘—ï¼å‡†ç¡®ç‡å’Œå®‰å…¨æ€§éƒ½æœ‰æ˜æ˜¾æå‡ã€‚")
            print(f"  âœ“ å»ºè®®é‡‡ç”¨DPOæ¨¡å‹è¿›è¡Œéƒ¨ç½²")
        elif acc_improvement > 0:
            print(f"  âœ“ DPOè®­ç»ƒæœ‰ä¸€å®šæ•ˆæœï¼Œå¯è€ƒè™‘è¿›ä¸€æ­¥ä¼˜åŒ–è®­ç»ƒå‚æ•°")
        else:
            print(f"  âš ï¸ DPOè®­ç»ƒæ•ˆæœä¸æ˜æ˜¾ï¼Œå»ºè®®æ£€æŸ¥ï¼š")
            print(f"     1. DPOæ•°æ®è´¨é‡ï¼ˆchosen vs rejectedå¯¹æ¯”åº¦ï¼‰")
            print(f"     2. è®­ç»ƒå‚æ•°ï¼ˆbetaå€¼ã€å­¦ä¹ ç‡ï¼‰")
            print(f"     3. æ•°æ®å¹³è¡¡æ€§")
        
        print("=" * 70)
    
    def _save_predictions(self, model_name: str, predictions: List[Dict]):
        """ä¿å­˜é¢„æµ‹ç»“æœ"""
        output_path = self.results_dir / f"{model_name.lower()}_predictions.json"
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(predictions, f, indent=2, ensure_ascii=False)
        print(f"âœ“ é¢„æµ‹ç»“æœå·²ä¿å­˜: {output_path}")
    
    def save_comparison_results(self, sft_results: Dict, dpo_results: Dict, 
                               comparison: Dict):
        """ä¿å­˜å¯¹æ¯”ç»“æœ"""
        output_path = self.results_dir / "model_comparison.json"
        
        results = {
            "sft_results": sft_results,
            "dpo_results": dpo_results,
            "comparison": comparison
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ“ å¯¹æ¯”ç»“æœå·²ä¿å­˜: {output_path}")


# ä¿ç•™æ—§çš„ç±»åä»¥å…¼å®¹æ€§
class ModelEvaluator(ImprovedModelEvaluator):
    """
    å…¼å®¹æ—§ç‰ˆæ¥å£çš„è¯„ä¼°å™¨åŒ…è£…ç±»
    ä¿æŒå‘åå…¼å®¹ï¼ŒåŒæ—¶æ”¯æŒæ–°åŠŸèƒ½ï¼ˆåŒ…æ‹¬å¤šæ ¸ä¼˜åŒ–ï¼‰
    """
    
    def __init__(self, num_workers: Optional[int] = None):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            num_workers: å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•°ï¼Œé»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°
        """
        super().__init__(num_workers=num_workers)
    
    def evaluate_baseline(self, core_llm, use_parallel: bool = True):
        """
        å…¼å®¹æ—§æ¥å£ï¼šè¯„ä¼°åŸºå‡†æ¨¡å‹ï¼ˆæ”¯æŒå¤šçº¿ç¨‹åŠ é€Ÿï¼‰
        
        Args:
            core_llm: æ ¸å¿ƒLLMå®ä¾‹
            use_parallel: æ˜¯å¦ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œï¼ˆé»˜è®¤Trueï¼‰
            
        Returns:
            (y_true, y_pred)
        """
        test_data = self.load_test_data()
        total_samples = len(test_data)
        
        print("\nè¯„ä¼°åŸºå‡†æ¨¡å‹ï¼ˆä½¿ç”¨æ‹’ç»å…³é”®è¯æ£€æµ‹ï¼‰...")
        
        if use_parallel and self.num_workers > 1:
            print(f"ğŸš€ ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç† ({self.num_workers} ä¸ªçº¿ç¨‹)")
            results = self._evaluate_baseline_parallel(core_llm, test_data)
        else:
            print(f"ğŸ“ ä½¿ç”¨å•çº¿ç¨‹å¤„ç†")
            results = self._evaluate_baseline_sequential(core_llm, test_data)
        
        # æå–ç»“æœ
        y_true = [r["true_label"] for r in results]
        y_pred = [r["pred_label"] for r in results]
        
        print("âœ“ åŸºå‡†æ¨¡å‹è¯„ä¼°å®Œæˆ")
        return y_true, y_pred
    
    def _evaluate_baseline_sequential(self, core_llm, test_data: List[Dict]) -> List[Dict]:
        """å•çº¿ç¨‹é¡ºåºè¯„ä¼°åŸºå‡†æ¨¡å‹"""
        results = []
        
        for i, sample in enumerate(test_data, 1):
            prompt = sample["prompt"]
            true_label = sample["label"]
            
            try:
                response = core_llm.chat(prompt)
                
                # åˆ¤æ–­æ˜¯å¦æ‹’ç»
                is_refused = any(keyword.lower() in response.lower() 
                               for keyword in self.refusal_keywords)
                pred_label = 1 if is_refused else 0
                
            except Exception as e:
                pred_label = 0
            
            results.append({
                "idx": i - 1,
                "true_label": true_label,
                "pred_label": pred_label
            })
            
            if i % 10 == 0:
                print(f"  è¿›åº¦: {i}/{len(test_data)}")
        
        return results
    
    def _evaluate_baseline_parallel(self, core_llm, test_data: List[Dict]) -> List[Dict]:
        """å¤šçº¿ç¨‹å¹¶è¡Œè¯„ä¼°åŸºå‡†æ¨¡å‹"""
        total_samples = len(test_data)
        results = []
        
        def process_sample(idx, sample):
            """å¤„ç†å•ä¸ªæ ·æœ¬"""
            prompt = sample["prompt"]
            true_label = sample["label"]
            
            try:
                response = core_llm.chat(prompt)
                
                # åˆ¤æ–­æ˜¯å¦æ‹’ç»
                is_refused = any(keyword.lower() in response.lower() 
                               for keyword in self.refusal_keywords)
                pred_label = 1 if is_refused else 0
                
            except Exception as e:
                pred_label = 0
            
            return {
                "idx": idx,
                "true_label": true_label,
                "pred_label": pred_label
            }
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
        with ThreadPoolExecutor(max_workers=self.num_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_idx = {
                executor.submit(process_sample, i, sample): i 
                for i, sample in enumerate(test_data)
            }
            
            # æ”¶é›†ç»“æœå¹¶æ˜¾ç¤ºè¿›åº¦
            completed = 0
            for future in as_completed(future_to_idx):
                result = future.result()
                results.append(result)
                completed += 1
                
                if completed % 50 == 0 or completed == total_samples:
                    print(f"  è¿›åº¦: {completed}/{total_samples} ({completed*100//total_samples}%)")
        
        # æŒ‰ç´¢å¼•æ’åº
        results.sort(key=lambda x: x["idx"])
        return results
    
    def evaluate_defense_system(self, defense_manager, use_parallel: bool = True):
        """
        å…¼å®¹æ—§æ¥å£ï¼šè¯„ä¼°é˜²å¾¡ç³»ç»Ÿï¼ˆæ”¯æŒå¤šçº¿ç¨‹åŠ é€Ÿï¼‰
        
        Args:
            defense_manager: é˜²å¾¡ç®¡ç†å™¨å®ä¾‹
            use_parallel: æ˜¯å¦ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œï¼ˆé»˜è®¤Trueï¼‰
            
        Returns:
            (y_true, y_pred, block_sources)
        """
        test_data = self.load_test_data()
        total_samples = len(test_data)
        
        print("\nè¯„ä¼°é˜²å¾¡ç³»ç»Ÿ...")
        
        if use_parallel and self.num_workers > 1:
            print(f"ğŸš€ ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç† ({self.num_workers} ä¸ªçº¿ç¨‹)")
            results = self._evaluate_defense_parallel(defense_manager, test_data)
        else:
            print(f"ğŸ“ ä½¿ç”¨å•çº¿ç¨‹å¤„ç†")
            results = self._evaluate_defense_sequential(defense_manager, test_data)
        
        # æå–ç»“æœ
        y_true = [r["true_label"] for r in results]
        y_pred = [r["pred_label"] for r in results]
        block_sources = [r["source"] for r in results]
        
        print("âœ“ é˜²å¾¡ç³»ç»Ÿè¯„ä¼°å®Œæˆ")
        return y_true, y_pred, block_sources
    
    def _evaluate_defense_sequential(self, defense_manager, test_data: List[Dict]) -> List[Dict]:
        """å•çº¿ç¨‹é¡ºåºè¯„ä¼°é˜²å¾¡ç³»ç»Ÿ"""
        results = []
        
        for i, sample in enumerate(test_data, 1):
            prompt = sample["prompt"]
            true_label = sample["label"]
            
            try:
                result = defense_manager.process(prompt)
                pred_label = 0 if result["success"] else 1
                source = result.get("source", "unknown")
                
            except Exception as e:
                pred_label = 0
                source = "error"
            
            results.append({
                "idx": i - 1,
                "true_label": true_label,
                "pred_label": pred_label,
                "source": source
            })
            
            if i % 10 == 0:
                print(f"  è¿›åº¦: {i}/{len(test_data)}")
        
        return results
    
    def _evaluate_defense_parallel(self, defense_manager, test_data: List[Dict]) -> List[Dict]:
        """å¤šçº¿ç¨‹å¹¶è¡Œè¯„ä¼°é˜²å¾¡ç³»ç»Ÿ"""
        total_samples = len(test_data)
        results = []
        
        def process_sample(idx, sample):
            """å¤„ç†å•ä¸ªæ ·æœ¬"""
            prompt = sample["prompt"]
            true_label = sample["label"]
            
            try:
                result = defense_manager.process(prompt)
                pred_label = 0 if result["success"] else 1
                source = result.get("source", "unknown")
                
            except Exception as e:
                pred_label = 0
                source = "error"
            
            return {
                "idx": idx,
                "true_label": true_label,
                "pred_label": pred_label,
                "source": source
            }
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
        with ThreadPoolExecutor(max_workers=self.num_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_idx = {
                executor.submit(process_sample, i, sample): i 
                for i, sample in enumerate(test_data)
            }
            
            # æ”¶é›†ç»“æœå¹¶æ˜¾ç¤ºè¿›åº¦
            completed = 0
            for future in as_completed(future_to_idx):
                result = future.result()
                results.append(result)
                completed += 1
                
                if completed % 50 == 0 or completed == total_samples:
                    print(f"  è¿›åº¦: {completed}/{total_samples} ({completed*100//total_samples}%)")
        
        # æŒ‰ç´¢å¼•æ’åº
        results.sort(key=lambda x: x["idx"])
        return results
    
    def calculate_metrics(self, y_true: List[int], y_pred: List[int]) -> Dict:
        """
        å…¼å®¹æ—§æ¥å£ï¼šè®¡ç®—æŒ‡æ ‡
        
        Args:
            y_true: çœŸå®æ ‡ç­¾
            y_pred: é¢„æµ‹æ ‡ç­¾
            
        Returns:
            æŒ‡æ ‡å­—å…¸
        """
        return self._calculate_overall_metrics(y_true, y_pred)
    
    def print_metrics(self, metrics: Dict, title: str):
        """
        å…¼å®¹æ—§æ¥å£ï¼šæ‰“å°æŒ‡æ ‡
        
        Args:
            metrics: æŒ‡æ ‡å­—å…¸
            title: æ ‡é¢˜
        """
        print("\n" + "=" * 70)
        print(title)
        print("=" * 70)
        
        print(f"\nå‡†ç¡®ç‡ (Accuracy):    {metrics['accuracy']:.4f}")
        print(f"ç²¾ç¡®ç‡ (Precision):   {metrics['precision']:.4f}")
        print(f"å¬å›ç‡ (Recall):      {metrics['recall']:.4f}")
        print(f"F1 åˆ†æ•° (F1 Score):   {metrics['f1_score']:.4f}")
        
        cm = metrics["confusion_matrix"]
        print(f"\næ··æ·†çŸ©é˜µ:")
        print(f"  TN (çœŸé˜´æ€§): {cm['TN']:3d}  |  FP (å‡é˜³æ€§): {cm['FP']:3d}")
        print(f"  FN (å‡é˜´æ€§): {cm['FN']:3d}  |  TP (çœŸé˜³æ€§): {cm['TP']:3d}")
        
        print(f"\nå…³é”®å®‰å…¨æŒ‡æ ‡:")
        print(f"  æ¼æŠ¥ç‡ (FNR): {metrics['false_negative_rate']:.4f} âš ï¸")
        print(f"  è¯¯æŠ¥ç‡ (FPR): {metrics['false_positive_rate']:.4f}")
        
        if metrics['false_negative_rate'] > 0.1:
            print(f"\nâš ï¸ è­¦å‘Š: æ¼æŠ¥ç‡è¿‡é«˜ ({metrics['false_negative_rate']:.2%})ï¼")
    
    def compare_systems(self, baseline_metrics: Dict, defense_metrics: Dict):
        """
        å…¼å®¹æ—§æ¥å£ï¼šå¯¹æ¯”ç³»ç»Ÿ
        
        Args:
            baseline_metrics: åŸºå‡†æ¨¡å‹æŒ‡æ ‡
            defense_metrics: é˜²å¾¡ç³»ç»ŸæŒ‡æ ‡
        """
        print("\n" + "=" * 70)
        print("ç³»ç»Ÿå¯¹æ¯”")
        print("=" * 70)
        
        metrics_names = ["accuracy", "precision", "recall", "f1_score", 
                        "false_negative_rate", "false_positive_rate"]
        
        print(f"\n{'æŒ‡æ ‡':<25} {'åŸºå‡†æ¨¡å‹':<15} {'é˜²å¾¡ç³»ç»Ÿ':<15} {'æ”¹å–„':<10}")
        print("-" * 70)
        
        for metric in metrics_names:
            baseline_val = baseline_metrics[metric]
            defense_val = defense_metrics[metric]
            
            if "rate" in metric:
                improvement = (baseline_val - defense_val) / baseline_val * 100 if baseline_val > 0 else 0
            else:
                improvement = (defense_val - baseline_val) / baseline_val * 100 if baseline_val > 0 else 0
            
            improvement_str = f"{improvement:+.1f}%"
            print(f"{metric:<25} {baseline_val:<15.4f} {defense_val:<15.4f} {improvement_str:<10}")
    
    def save_results(self, baseline_metrics: Dict, defense_metrics: Dict,
                    baseline_predictions: Tuple, defense_predictions: Tuple) -> Dict:
        """
        å…¼å®¹æ—§æ¥å£ï¼šä¿å­˜ç»“æœ
        
        Args:
            baseline_metrics: åŸºå‡†æŒ‡æ ‡
            defense_metrics: é˜²å¾¡æŒ‡æ ‡
            baseline_predictions: åŸºå‡†é¢„æµ‹
            defense_predictions: é˜²å¾¡é¢„æµ‹
            
        Returns:
            ç»“æœå­—å…¸
        """
        results = {
            "baseline": baseline_metrics,
            "defense": defense_metrics,
            "baseline_predictions": {
                "y_true": baseline_predictions[0],
                "y_pred": baseline_predictions[1],
            },
            "defense_predictions": {
                "y_true": defense_predictions[0],
                "y_pred": defense_predictions[1],
                "block_sources": defense_predictions[2] if len(defense_predictions) > 2 else [],
            }
        }
        
        output_path = self.results_dir / "evaluation_results.json"
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ“ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        return results


if __name__ == "__main__":
    print("=" * 70)
    print("å¢å¼ºç‰ˆæ¨¡å‹è¯„ä¼°æ¨¡å—")
    print("=" * 70)
    print("\næ­¤æ¨¡å—éœ€è¦é…åˆ main.py --evaluate ä½¿ç”¨")
    print("è¯·è¿è¡Œ: python main.py --evaluate")
    print("\næ–°å¢åŠŸèƒ½:")
    print("  âœ“ æŒ‰æ”»å‡»ç±»åˆ«ç»†åˆ†è¯„ä¼°")
    print("  âœ“ æŒ‰éš¾åº¦çº§åˆ«ç»†åˆ†è¯„ä¼°")
    print("  âœ“ è¯¦ç»†é”™è¯¯åˆ†æ")
    print("  âœ“ SFT vs DPO å¯¹æ¯”")
    print("  âœ“ ç½®ä¿¡åº¦ç»Ÿè®¡")
